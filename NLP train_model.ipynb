{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification: Bert-Based Uncased with PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This repository includes a Jupyter Notebook that incorporates a sentiment analysis model using the Bert-Based Uncased architecture, best described in the academic paper \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\" by Devlin, Jacob, et al. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv, Cornell University, 11 Oct. 2018, https://arxiv.org/abs/1810.04805. Accessed 30 June 2024. \n",
    "\n",
    "<img src='https://nlp.gluon.ai/_images/bert-sentence-pair.png' width='800'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this project, I will be using Hugging Face's Bert-Based Uncased encoder architecture to train a sentiment analysis model on an amazon product review dataset. This model will allow many different functions of a amazon's business such as marketing or product to analyze customer sentiment of the products they sell with high precision and speed to allow stakeholders to focus on the decision making process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pyarrow as pa\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import opendatasets as od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \".\\amazon-product-reviews\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "od.download(\"https://www.kaggle.com/datasets/mahmudulhaqueshawon/amazon-product-reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is  the best apps acording to a bunch of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a pretty good version of the game for ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is a really . there are a bunch of levels...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a silly game and can be frustrating, b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is a terrific game on any pad. Hrs of fun...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  label\n",
       "0  This is  the best apps acording to a bunch of ...      1\n",
       "1  This is a pretty good version of the game for ...      1\n",
       "2  this is a really . there are a bunch of levels...      1\n",
       "3  This is a silly game and can be frustrating, b...      1\n",
       "4  This is a terrific game on any pad. Hrs of fun...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"amazon-product-reviews/amazon.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(row):\n",
    "\n",
    "    text = row['Text']\n",
    "    text = str(text)\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    encodings = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "    label = 0\n",
    "    if row['label'] == 1:\n",
    "        label += 1\n",
    "\n",
    "    encodings['label'] = label\n",
    "    encodings['Text'] = text\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2023, 2003, 1037, 7099, 3319, 1997, 1037, 3185, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 1, 'Text': 'this is a sample review of a movie.'}\n"
     ]
    }
   ],
   "source": [
    "print(process_data({\n",
    "    'Text': 'this is a sample review of a movie.',\n",
    "    'label': 1\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "\n",
    "for i in range(len(df[:1000])):\n",
    "    processed_data.append(process_data(df.iloc[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(processed_data)\n",
    "\n",
    "train_df, valid_df = train_test_split(\n",
    "    new_df,\n",
    "    test_size=0.2,\n",
    "    random_state=2022\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hg = Dataset(pa.Table.from_pandas(train_df))\n",
    "valid_hg = Dataset(pa.Table.from_pandas(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir=\"./result\",\n",
    "                                  eval_strategy=\"epoch\",\n",
    "                                 num_train_epochs = 4,\n",
    "                                 learning_rate = 2e-5,\n",
    "                                 weight_decay=0.01,\n",
    "                                 save_strategy='epoch',\n",
    "                                 load_best_model_at_end=True,\n",
    "                                 logging_strategy='epoch')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_hg,\n",
    "    eval_dataset=valid_hg,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cincy\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 04:33, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.343100</td>\n",
       "      <td>0.142146</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.154600</td>\n",
       "      <td>0.062358</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.075180</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.037500</td>\n",
       "      <td>0.075360</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.1469484955072403, metrics={'train_runtime': 274.8339, 'train_samples_per_second': 11.643, 'train_steps_per_second': 1.455, 'total_flos': 210488844288000.0, 'train_loss': 0.1469484955072403, 'epoch': 4.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.062357522547245026,\n",
       " 'eval_accuracy': 0.985,\n",
       " 'eval_runtime': 5.1033,\n",
       " 'eval_samples_per_second': 39.19,\n",
       " 'eval_steps_per_second': 4.899,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "new_model = AutoModelForSequenceClassification.from_pretrained('./model/').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: me and the kids love this and play it everywhere waiting for our food while i'm shopping its a must have to keep everyone busy\n",
      "Actual Label: 1\n",
      "Prediction: 1 with probability 0.9556\n",
      "\n",
      "Input: By boys promised I would like Angry Birds, and thy are right .It's A very fun game It's hard to put down once you get started.and easy to lose track of time when your playing.best game ever\n",
      "Actual Label: 1\n",
      "Prediction: 1 with probability 0.9508\n",
      "\n",
      "Input: I downloaded this right after Christmas and have a hard time putting down, it's so addictive! My daughter has all of the Angry birds games on both the pc and my laptop, but she's not allowed to play on my Kindle. Why does this game draw me in so much? Th\n",
      "Actual Label: 1\n",
      "Prediction: 1 with probability 0.9498\n",
      "\n",
      "Input: Who doesn't like angry birds? It's free and after you finish each episode, the next challenge is to do them all perfectly. It's harder than you might think.\n",
      "Actual Label: 1\n",
      "Prediction: 1 with probability 0.9505\n",
      "\n",
      "Input: I think that my 5 year old loves this game the most of all of us in the family. She always says Bingo when she gets the high score in a level. I would recommend this game to one and all!!\n",
      "Actual Label: 1\n",
      "Prediction: 1 with probability 0.9534\n",
      "\n",
      "Input: I had this already on my IPhone and I really like it so I knew on a bigger screen would be great, so I was right. Thanks for the free app I like it alot.\n",
      "Actual Label: 1\n",
      "Prediction: 1 with probability 0.9406\n",
      "\n",
      "Input: This is another game that will keep a child entertained for hours if you let them. especially good for waiting rooms .\n",
      "Actual Label: 1\n",
      "Prediction: 1 with probability 0.9512\n",
      "\n",
      "Input: This is one of my son's favorite games to play and he loves it. Really like that I could download this one\n",
      "Actual Label: 1\n",
      "Prediction: 1 with probability 0.9479\n",
      "\n",
      "Input: This is a great &quot;bejewled&quot; type of game. The graphics are great and the combos keep things interesting. Unlike a lot of other match 3 games, the game feels like it is progressing and going somewhere, which I really liked. There are a bunch of l\n",
      "Actual Label: 1\n",
      "Prediction: 1 with probability 0.9356\n",
      "\n",
      "Input: I absolutely loved it and tried the Space Birds, thought not as addicting, it is okay. Good when I'm not online.\n",
      "Actual Label: 1\n",
      "Prediction: 1 with probability 0.9446\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_prediction(text):\n",
    "    encoding = new_tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "    encoding = {k: v.to(trainer.model.device) for k, v in encoding.items()}\n",
    "\n",
    "    outputs = new_model(**encoding)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(logits.squeeze().cpu())\n",
    "    probs = probs.detach().numpy()\n",
    "    label = np.argmax(probs, axis=-1)\n",
    "    \n",
    "    if label == 1:\n",
    "        return {\n",
    "            'label': 1,\n",
    "            'probability': probs[1]\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'label': 0,\n",
    "            'probability': probs[0]\n",
    "        }\n",
    "\n",
    "# Counter to limit the number of entries processed\n",
    "counter = 0\n",
    "max_entries = 10\n",
    "\n",
    "# Iterate over each row in the 'Text' column and print the result\n",
    "for index, row in valid_df.iterrows():\n",
    "    if counter >= max_entries:\n",
    "        break\n",
    "    input_string = row['Text']\n",
    "    actual_label = row['label']\n",
    "    result = get_prediction(input_string)\n",
    "    print(f\"Input: {input_string}\")\n",
    "    print(f\"Actual Label: {actual_label}\")\n",
    "    print(f\"Prediction: {result['label']} with probability {result['probability']:.4f}\")\n",
    "    print()\n",
    "    counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "173fe52379437b78f95c8980b8ee9f2930fd7b56889ab31a72735475ddc10c81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
